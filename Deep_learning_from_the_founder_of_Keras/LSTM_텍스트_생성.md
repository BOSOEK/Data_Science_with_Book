# LSTM 텍스트 생성
> 순환 신경망으로 시퀀스 데이터 생성

## 1. 시퀀스 생성  
딥러닝에서 시퀀스를 생성하는 일반적인 방법은 이전 토큰을 입력(조건 데이터)으로 __시퀀스의 다음 1개 or 몇개의 토큰을 예측__ 하는 것이다.  
> 이렇게 이전 토큰이 주어졌을 때 다음 __토큰의 확률을 모델링하는 네트워크를 언어 모델__ 이라고 한다.

## 2. 샘플링 전략의 중요성
텍스트 생성시 다음 글자를 선택하는 방법은 아주 중요하며 대표적으로 두가지가 있다.
* __탐욕적 샘플링__ : 항상 가장 높은 확률을 가진 글자를 선택하며, 반복적이고 예측 가능한 문자열을 만든다.
* __확률적 샘플링__ : 다음 글자 확률 분포에서 샘플링하는 과정에 무작위성을 주입한다.
> 샘플링 과정에서 무작위성은 매우 중요하다.    
> 무작위 성이 크면 __창의적인 시퀀스__ 를 만들지만    
> 무작위 성이 너무 크면 __모든 글자의 확률이 같아지며__ 당연하게도 연관성 있는 텍스트를 만들지 못한다. 이를 __확률 분포가 최대의 엔트로피를 가진다고 한다.__   
> 반대로 무작위 성이 작으면 __더 실제같은__ 예상 가능한 시퀀스를 만들지만,  
> 무작위 성이 너무 작으면 __무작위성이 없어져서 논리적이지 않아진다.__ 이를 __확률 분포가 최소의 엔트로피를 가진다고 한다.__

즉, 토큰을 샘플링할때는 모델이 만든 __출력에 집중하는 것과 무작위성을 주입하는 것 사이에 균형을 맞춰야 한다__  
그리고 이를 위해 __소프트맥스 온도__ 파라미터를 사용한다
  
> __소프트맥스 온도__ 파라미터는 샘플링에 사용되는 확률 분포의 엔트로피를 나타낸다.