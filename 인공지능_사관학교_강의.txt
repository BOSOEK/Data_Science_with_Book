* 인공지능 사관학교 인공지능 강의 영상 *

머신러닝 : 입력 데이터에서 관계를 학습하고 새로운 사례에서 학습한 내용 사용
머신러닝에 필요한 점 : 입력 데이터, 예상 출력값, 알고리즘 정확도 측정법
표현 : 데이터를 대표하게 하거나 데이터를 부호화 하기 위해 데이터를 색다르게 살펴보는 방법
딥러닝 : 머신러닝의 하위 분야로, 표현들을 점점 의미있게 만들어 가는 연속 계층들을 학습하게 하는 데 중점을 두고 데이터 표현을 학습하는 방법
* 신경망 : 생물학에서 차용(뉴런), 뇌를 이해한 데서 얻은 영감을 통해 부분적으로 개발
1. 뉴런은 네트워크를 형성한다
2. 여러 뉴런에 전달되는 신호의 합이 일정크기(임곗값)을 넘지 않으면 뉴런은 반응하지 않는다
3. 여러 뉴런에 전달되는 신호의 합이 일정 크기 임곗값을 넘으면 뉴런이 반응하며, 다른 뉴런에 일정 강도의 신호를 전달한다.
4. 2와 3에서 여러 뉴런에 전달되는 신호의 합은 신호마다 가중치가 다르다.
-> 즉, 신경세포를 수학적 추상화로 일정단위의 인위적인 네트워크로 표현한것이 신경망이며 딥러닝의 기초이다
*학습 : 신경망에서 모든 계층의 가중치에 대한 값 집합을 찾아, 새로운 입력값이 목표한 값에 올바르게 사상할 수 있도록 하는 반복작업
* 손실 함수 : 예측값과 실제 출력 값의 거리 점수를 계산하여 신경망이 얼마나 잘 수행되었는지 계산
-> 실제로는 손실함수를 피드백 신호로써 손실 함수 값을 낮추기 위해 가중치 값을 조절한다.

//2-1강. 머신러닝의 분류
*지도학습 : 레이블된 훈련 데이터를 활용해 모델을 학습시킨후, 새로운 데이터에 대해 예측값을 출력하는 것
- 분류 : 개별 클래스 레이블이 있는 지도학습(말 그대로 새로운 데이터를 분류함)
- 회귀 : 연속적인 출력값을 예측한다.(입출력 데이터의 차이를 학습함)
*비지도 학습 : 레이블되지 않은 데이터나 구조를 알 수 없는 데이터에서 의미있는 정보를 추출하거나 데이터 구조를 탐색할 수 있다.
- 군집 : 그룹 정보를 서브그룹이나 클러스터로 조직하는 탐색적 데이터 분석 기법(군집은 정보를 조직화하고 데이터에서 의미있는 관계를 찾을 수 있다)
- 차원 축소 : 고차원의 데이터를 저차원으로 축소하는 기법(비지도 차원 축소 기법) -> 대부분의 정보를 유지마혀 더 작은 차원의 부분 공간으로 데이터 압축
*강화 학습 : 환경이란 개념과 상호작용하며 에이전트의 성능을 향상시키는 것입니다.
- 행동에 따른 보상과 벌로 보상이 최대화 되는 행동을 학습한다.

//2-2강. 기초수학[1]
* 통계학 : 수치 데이터의 수집, 분석, 해석, 표현 등을 다루는 수학의 한 분야(기술 통계학과 추론 통계학으로 나뉨)
- 기술 통계학(연속형 데이터와 범주형 데이터로 나뉨)
1. 연속형 데이터 : 키, 나이, 가격등의 데이터로 평균/표준편차를 구하는 것
2. 범주형 데이터 : 이름, 종족, 성별등의 데이터로 빈도/백분율을 그하는 것
- 추론 통계학 : 가설 검정, 수치로 되어 있는 특징 계산, 각 데이터간의 상관관계를 분석
* 통계 모델링 : 데이터에 통계학을 적용하여 변수의 유의성을 분석함으로써, 방대한 양의 데이터에 숨겨진 특징을 찾아내는 것을 의미
- 통계 모델(수학적 모델) : 변수들로 이뤄진 수학식을 계산해 실제 값을 추정하는 방식
-> 통계모델은 여러 가정을 가지며 이 가정들은 가운데가 볼록한 대칭형 종모양 그래피인 확률 분포(정규 분포)를 따른다
- 확률 분포 : 종모양의 분포로 최빈값, 중앙값, 평균을 가짐(정규분포는 최빈값, 중앙값, 평균이 같은 값을 가지며 그 외의 분포는 각각의 값이 다르다)
* 변량의 측정 
- 변량 : 수치 또는 변수
- 산포 : 데이터의 변량의로 데이터가 얼마나 중심으로 모이지 않고 흩어져 있는지를 나타냄
- 분산 : 변량을 측정하는 수치중 하나로, 평균과의 거리를 제곱한 값의 평균
-> 분산에 제곱근을 한 값을 표준편차라고 한다
* 사분위수 : 데이터를 사등분 한 지점을 표현한 지표
- 사분위수는 데이터 구성을 전체적으로 살펴보고자 할 때 사용, 데이터의 이상치 탐색과 중심위치 및 분포를 빠르게 파악할 수 있다는 강점이 있다.

//2-3강. 기초수학[2]
* 목적 함수 : 모든 점에서(데이터에서) 생기는 오차의 합이 가능한 작아지는 함수
- 목적 함수 사용법 : 각각의 데이터의 실제 결과값과 예측한 결과값의 오차를 제곱해서 모두 더하고, 그것의 2/1을 해준다
* 최적합 문제 : 목적함수의 파라미터가 가장 작아지는 값을 구한다.
* 경사 하강법 : 목적함수의 값을 최소화 시키기 위해 마치 경사를 내려가듯 최소값을 찾는 기법


