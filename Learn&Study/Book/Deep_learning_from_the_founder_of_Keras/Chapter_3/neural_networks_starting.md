# 3장. 신경망 시작하기
> 이 장에서는 층, 네트워크, 목적함수, 옵티마이저 같은 신경망의 핵심 구성 요소들을 자세히 살펴보고 이진 분류, 다중 분류, 스칼라 값을 예측하는 회귀 문제를 푼다
*** 

## 1. 신경망의 구조
* ### 신경망 훈련 요소
    * __네트워크(모델)__ 를 구성하는 __층__
    * __입력 데이터__ 와 그에 상응하는 __타깃__
    * 학습에 사용할 피드백 신호를 정의하는 __손실 함수__
    * 학습 진행 방식을 결정하는 __옵티마이저__
* ### 층 : 딥러닝의 구성 단위 
    > 하나 이상의 텐서를 입력받아 하나 이상의 텐서를 출력하는 데이터 처리 모듈로, __가중치__ (확률적 경사 하강법에 의해 학습되는 하나 이상의 텐서로 네트워크가 학습한 __지식__ 이 담김)를 가진다.
    
    > 층별로 적절한 텐서 포맷 & 데이터 처리 방식이 다르다.   
    * 벡터 데이터(2D 텐서) : 완전 연결층, 밀집 층 등의 밀집 연결층(케라스의 Dense 클래스)
    * 시퀀스 데이터(3D 텐서) : LSTM 같은 순환 층
    * 이미지 데이터(4D 텐서) : 2D 합성곱 층(Conv2D 클래스)
    > 층 호환성 : 각 층이 특정 크기의 입력텐서만 받고 특정 크기의 출력 텐서를 반환하는 것.
* ### 모델 : 층의 네트워크
    > 딥러닝 모델은 층으로 만든 __비순환 유향 그래프__ (한 노드에서 자신에게 돌아오는 경로가 없는 그래프)다.
    * 네트워크 구조는 __가설 공간__ 을 정의한다.
    > 네트워크 구조를 선택함으로 _가능성 있는 공간(가설공간)__ 을 입력 데이터에서 출력 데이터로 매핑하는 일련의 특정 텐서 연산으로 제한하며, 딥러닝 시 이런 가중치 텐서의 좋은 값을 찾아야 한다.
* ### 손실 함수와 옵티마이저 : 학습 과정을 조절하는 열쇠
    * __손실 함수(목적함수)__ : 훈련하는 동안 최소화될 값. 주어진 문제에 대한 성공 지표이다.
    * __옵티마이저__ : 손실 함수를 기반으로 네트워크가 어떻게 업데이트 될지 결정. 특정 종류의 확률적 경사 하강법 구현한다.
    > 여러 출력의 신경망은 여러 손실 함수를 가질 수 있지만 손실이 여러개인 네트워크에서는 모든 손실이 (평균내서)하나의 스칼라 양으로 합쳐짐.
    
    > 즉, 목적함수를 제대로 선택하지 않으면 부수적인 효과가 발생한다.

    > 일반적인 문제에서는 손실 함수 지침이 있다.
    * 2개의 클래스가 있는 분류 문제 : 이진 크로스엔트로피
    * 여러 개의 클래스가 있는 분류 문제 : 범주형 크로스엔트로피
    * 회귀 문제 : 평균 제곱 오차
    * 시퀀스 학습 문제 : CTC(Connection Temporal Classification)