# 3장. 신경망 시작하기
> 이 장에서는 층, 네트워크, 목적함수, 옵티마이저 같은 신경망의 핵심 구성 요소들을 자세히 살펴보고 이진 분류, 다중 분류, 스칼라 값을 예측하는 회귀 문제를 푼다
*** 

## 1. 신경망의 구조
* ### 신경망 훈련 요소
    * __네트워크(모델)__ 를 구성하는 __층__
    * __입력 데이터__ 와 그에 상응하는 __타깃__
    * 학습에 사용할 피드백 신호를 정의하는 __손실 함수__
    * 학습 진행 방식을 결정하는 __옵티마이저__
* ### 층 : 딥러닝의 구성 단위 
    > 하나 이상의 텐서를 입력받아 하나 이상의 텐서를 출력하는 데이터 처리 모듈로, __가중치__ (확률적 경사 하강법에 의해 학습되는 하나 이상의 텐서로 네트워크가 학습한 __지식__ 이 담김)를 가진다.
    
    > 층별로 적절한 텐서 포맷 & 데이터 처리 방식이 다르다.   
    * 벡터 데이터(2D 텐서) : 완전 연결층, 밀집 층 등의 밀집 연결층(케라스의 Dense 클래스)
    * 시퀀스 데이터(3D 텐서) : LSTM 같은 순환 층
    * 이미지 데이터(4D 텐서) : 2D 합성곱 층(Conv2D 클래스)
    > 층 호환성 : 각 층이 특정 크기의 입력텐서만 받고 특정 크기의 출력 텐서를 반환하는 것.
* ### 모델 : 층의 네트워크
    > 딥러닝 모델은 층으로 만든 __비순환 유향 그래프__ (한 노드에서 자신에게 돌아오는 경로가 없는 그래프)다.
    * 네트워크 구조는 __가설 공간__ 을 정의한다.
    > 네트워크 구조를 선택함으로 _가능성 있는 공간(가설공간)__ 을 입력 데이터에서 출력 데이터로 매핑하는 일련의 특정 텐서 연산으로 제한하며, 딥러닝 시 이런 가중치 텐서의 좋은 값을 찾아야 한다.
* ### 손실 함수와 옵티마이저 : 학습 과정을 조절하는 열쇠
    * __손실 함수(목적함수)__ : 훈련하는 동안 최소화될 값. 주어진 문제에 대한 성공 지표이다.
    * __옵티마이저__ : 손실 함수를 기반으로 네트워크가 어떻게 업데이트 될지 결정. 특정 종류의 확률적 경사 하강법 구현한다.
    > 여러 출력의 신경망은 여러 손실 함수를 가질 수 있지만 손실이 여러개인 네트워크에서는 모든 손실이 (평균내서)하나의 스칼라 양으로 합쳐짐.
    
    > 즉, 목적함수를 제대로 선택하지 않으면 부수적인 효과가 발생한다.

    > 일반적인 문제에서는 손실 함수 지침이 있다.
    * 2개의 클래스가 있는 분류 문제 : 이진 크로스엔트로피
    * 여러 개의 클래스가 있는 분류 문제 : 범주형 크로스엔트로피
    * 회귀 문제 : 평균 제곱 오차
    * 시퀀스 학습 문제 : CTC(Connection Temporal Classification)
***
# 문제 풀이
* ### [영화 리뷰 분류 : 이진 분류 예제](https://github.com/BOSOEK/Machine_Learning/blob/main/Learn%26Study/Book/Deep_learning_from_the_founder_of_Keras/Chapter_3/%EC%98%81%ED%99%94%EB%A6%AC%EB%B7%B0%EB%B6%84%EB%A5%98_%EC%9D%B4%EC%A7%84%EB%B6%84%EB%A5%98%EC%98%88%EC%A0%9C.ipynb)
* ### [뉴스 기사 분류 : 다중 분류 문제](https://github.com/BOSOEK/Machine_Learning/blob/main/Learn%26Study/Book/Deep_learning_from_the_founder_of_Keras/Chapter_3/%EB%89%B4%EC%8A%A4%EA%B8%B0%EC%82%AC%EB%B6%84%EB%A5%98_%EB%8B%A4%EC%A4%91%EB%B6%84%EB%A5%98%EB%AC%B8%EC%A0%9C.ipynb)
* ### [주택 가격 예측 : 회귀 문제](https://github.com/BOSOEK/Machine_Learning/blob/main/Learn%26Study/Book/Deep_learning_from_the_founder_of_Keras/Chapter_3/%EC%A3%BC%ED%83%9D%EA%B0%80%EA%B2%A9%EC%98%88%EC%B8%A1_%ED%9A%8C%EA%B7%80%EB%AC%B8%EC%A0%9C.ipynb)
***
# 정리
* #### 신경망에 데이터를 주입하기 전에 전처리를 해야 한다.
* #### 데이터에 범위(스케일)가 다른 특성(feature)가 있다면 전처리 단계에서 특성을 독립적으로 조정해야 한다.
* #### 훈련이 진행되면서 점차 신경망이 과대적합 되고 새로운 데이터에 나쁜 결과를 얻는다
* #### 훈련 데이터가 많지 않으면 과대적합을 피하기 위해 1 or 2개의 은닉층을 가진 신경망을 사용한다.
* #### 데이터가 많은 범주로 나뉘었을 때 중간층이 작으면 정보의 병목이 생길 수 잇다.
* #### 회귀는 분류와 다른 손실함수 & 평가 지표를 사용한다
* #### 적은 데이터를 사용할 때는 __K-겹 검증이 모델__ 의 신뢰 평가를 도와준다.