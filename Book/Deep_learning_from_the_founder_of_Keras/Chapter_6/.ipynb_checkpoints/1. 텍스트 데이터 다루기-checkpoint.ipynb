{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aa43bc1e-041b-4135-a583-ccac49451fe2",
   "metadata": {},
   "source": [
    "# 1. 텍스트 데이터 다루기\n",
    "> 신경망의 학습을 위해 텍스트를 수치형 텐서로 변환하는 과정(__텍스트 벡터화__)가 필요하다.\n",
    "\n",
    "### 텍스트 벡터화 방식\n",
    "* 텍스트를 단어로 나누고 각 단어를 하나의 벡터로 변환\n",
    "* 텍스트를 문자로 나누고 각 문자를 하나의 벡터로 변환\n",
    "* 텍스트에서 단어난 문자의 n-그램을 추출, 각 n-그램을 하나의 벡터로 변환한다. n-그램은 연속된 단어 & 문자의 그룹으로 텍스트에서 단어나 문자를 하나씩 이동하면서 추출\n",
    "\n",
    "> 토큰 : 텍스트를 나누는 단위(단어, 문자)\n",
    "> 토큰화 : 텍스트를 토큰으로 나누는 작업\n",
    "\n",
    "## 토큰과 벡터를 연결하는 방법\n",
    "#### 1. 원-핫 인코딩\n",
    "> 모든 단어에 정수 인덱스 부여후 정수 인덱스 i를 크기가 어휘 사전의 크기인 이진 벡터로 변환한다.(원소 하나만 1이고 나머지 0)\n",
    "#### 2. 토큰 임베딩(단어 임베딩)\n",
    "> 실수형 밀집 벡터로 워-핫 인코딩에 비해 차원이 적다.\n",
    "    \n",
    "> 단어 임베딩을 만드는  방법   \n",
    "   * 관심 대상인 문제와 함께 단어 임베딩 학습. 이런 경우 랜덤한 단어 벡터로 시작해서 신경망의 가중치를 학습하는 것과 같은 방식으로 단어 벡터를 학습.\n",
    "   * 풀려는 문제가 아닌 다른 머신 러닝 작업에서 미리 계산된 단어 임베딩을 로드. 이를 __사전 훈련된 단어 임베딩__ 이라고한다.\n",
    "   \n",
    "> Embedding 층으로 단어 임베딩을 만들때 임베딩 공간에서 비슷한 뜻의 단어는 가까이 다른 뜻의 단어는 먼 곳에 배치한다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "59530ce2-af41-4a22-b59b-1bce203bb1fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedding 층의 객체 생성하기\n",
    "from keras.layers import Embedding\n",
    "\n",
    "embedding_layer = Embedding(1000, 64) # Embedding 층은 적어도 2개의 매개변수를 받는다\n",
    "# 가능한 토큰의 개수(1000으로 단언 인덱스 최대값 + 1와 임베딩차원(64)이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f9571a1e-3f0a-4ffc-8399-c606aba53082",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
      "17465344/17464789 [==============================] - 1s 0us/step\n",
      "17473536/17464789 [==============================] - 1s 0us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<__array_function__ internals>:5: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "C:\\Users\\김보석\\AppData\\Roaming\\Python\\Python38\\site-packages\\keras\\datasets\\imdb.py:155: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  x_train, y_train = np.array(xs[:idx]), np.array(labels[:idx])\n",
      "C:\\Users\\김보석\\AppData\\Roaming\\Python\\Python38\\site-packages\\keras\\datasets\\imdb.py:156: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  x_test, y_test = np.array(xs[idx:]), np.array(labels[idx:])\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import imdb\n",
    "from keras import preprocessing\n",
    "\n",
    "max_features = 10000  # 특성으로 사용할 단어의 수\n",
    "maxlen = 20  # 사용할 텍스트의 길이\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words = max_features)  # 정수 리스트로 데이터 로드\n",
    "\n",
    "x_train = preprocessing.sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = preprocessing.sequence.pad_sequences(x_test, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dfe6d5ce-8b68-44ec-8c67-4ed811a5ac9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 20, 8)             80000     \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 160)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 161       \n",
      "=================================================================\n",
      "Total params: 80,161\n",
      "Trainable params: 80,161\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "625/625 [==============================] - 28s 5ms/step - loss: 0.6859 - acc: 0.5693 - val_loss: 0.6287 - val_acc: 0.6944\n",
      "Epoch 2/10\n",
      "625/625 [==============================] - 1s 2ms/step - loss: 0.5790 - acc: 0.7484 - val_loss: 0.5336 - val_acc: 0.7280\n",
      "Epoch 3/10\n",
      "625/625 [==============================] - 2s 3ms/step - loss: 0.4774 - acc: 0.7882 - val_loss: 0.5024 - val_acc: 0.7440\n",
      "Epoch 4/10\n",
      "625/625 [==============================] - 3s 5ms/step - loss: 0.4197 - acc: 0.8158 - val_loss: 0.4953 - val_acc: 0.7518\n",
      "Epoch 5/10\n",
      "625/625 [==============================] - 3s 5ms/step - loss: 0.3846 - acc: 0.8355 - val_loss: 0.4953 - val_acc: 0.7548\n",
      "Epoch 6/10\n",
      "625/625 [==============================] - 3s 5ms/step - loss: 0.3632 - acc: 0.8438 - val_loss: 0.4999 - val_acc: 0.7542\n",
      "Epoch 7/10\n",
      "625/625 [==============================] - 3s 5ms/step - loss: 0.3380 - acc: 0.8583 - val_loss: 0.5058 - val_acc: 0.7514\n",
      "Epoch 8/10\n",
      "625/625 [==============================] - 3s 5ms/step - loss: 0.3197 - acc: 0.8709 - val_loss: 0.5128 - val_acc: 0.7524\n",
      "Epoch 9/10\n",
      "625/625 [==============================] - 3s 5ms/step - loss: 0.2976 - acc: 0.8787 - val_loss: 0.5220 - val_acc: 0.7470\n",
      "Epoch 10/10\n",
      "625/625 [==============================] - 3s 5ms/step - loss: 0.2910 - acc: 0.8835 - val_loss: 0.5323 - val_acc: 0.7472\n"
     ]
    }
   ],
   "source": [
    "# IMDB 데이터에 Embedding 층과 분류기 사용하기\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Flatten, Dense, Embedding\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(10000, 8, input_length=maxlen))  \n",
    "# 나중에 임베딩된 입력을 Flatten 층에서 펼치기 위해 Embedding 층에 input_length를 지정\n",
    "# Embedding 층의 출력 크기는 (samples, maxlen, 8) 이 된다\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='rmsprop', loss = 'binary_crossentropy', metrics=['acc'])\n",
    "model.summary()\n",
    "\n",
    "history = model.fit(x_train, y_train,\n",
    "                   epochs=10,\n",
    "                   batch_size=32,\n",
    "                   validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aced2463-30bb-4004-89f3-ec2dcef30e5e",
   "metadata": {},
   "source": [
    "### 사전 훈련된 단어 임베딩 사용하기\n",
    "케라스 임베딩 층을 위해 미리 계산된 단어 임베딩 DB의 일부로 __Word2vec, GloVe__ 이다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baab666a-2467-4708-9982-e07c77a39053",
   "metadata": {},
   "source": [
    "### 모든 내용을 적용하기 : 원본 텍스트에서 단어 임베딩까지\n",
    "> IMDB 데이터셋을 받고 압축 해제후 훈련 데이터 리스트와 리뷰 레이블 리스트를 만듦"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c12f4f64-d529-4c9e-b1de-5e4bbdce3b5a",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] 지정된 경로를 찾을 수 없습니다: './datasets/aclImdb\\\\test\\\\pos'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-9fc4daa778b4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mlabel_type\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'neg'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'pos'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[0mdir_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_dir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel_type\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mfname\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdir_name\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mfname\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'.txt'\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m             \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdir_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'utf8'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] 지정된 경로를 찾을 수 없습니다: './datasets/aclImdb\\\\test\\\\pos'"
     ]
    }
   ],
   "source": [
    "import os \n",
    "\n",
    "imdb_dir ='./datasets/aclImdb'\n",
    "train_dir = os.path.join(imdb_dir, 'test')\n",
    "\n",
    "labels = []\n",
    "texts = []\n",
    "\n",
    "for label_type in ['neg', 'pos'] :\n",
    "    dir_name = os.path.join(train_dir, label_type)\n",
    "    for fname in os.listdir(dir_name) :\n",
    "        if fname[-4:] == '.txt' :\n",
    "            f = open(os.path.join(dir_name, fname), encoding='utf8')\n",
    "            texts.append(f.read())\n",
    "            f.close()\n",
    "            if label_type == 'neg' :\n",
    "                labels.append(0)\n",
    "            else :\n",
    "                labels.append(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2fd86ae-f7ad-46b5-a195-f1055f6c94d7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
